#!/usr/bin/env python3
"""
Daily Tech Digest Agent
========================
ä½¿ç”¨ Claude Agent SDK æ¯å¤©å®šæ—¶ç”ŸæˆæŠ€æœ¯æ—¥æŠ¥å¹¶å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å·

åŠŸèƒ½ï¼š
1. èšåˆ HN/ProductHunt/Twitter æ•°æ®
2. AI åˆ†æè¶‹åŠ¿å¹¶ç”Ÿæˆæ—¥æŠ¥
3. è‡ªåŠ¨å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å·

ä½¿ç”¨æ–¹æ³•ï¼š
    python tech_digest_agent.py              # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    python tech_digest_agent.py --schedule   # å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©8:00ï¼‰
    python tech_digest_agent.py --test       # æµ‹è¯•æ¨¡å¼ï¼ˆä¸å‘å¸ƒï¼‰

ç¯å¢ƒå˜é‡ï¼ˆæˆ– .env æ–‡ä»¶ï¼‰ï¼š
    ANTHROPIC_API_KEY=sk-ant-xxx
    WECHAT_APP_ID=wxxxxxxxxxxx
    WECHAT_APP_SECRET=xxxxxxxxxx
"""

import os
import json
import re
import logging
import argparse
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any
from pathlib import Path

import anthropic
import requests
import schedule
import time
from PIL import Image, ImageDraw, ImageFont
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tech_digest.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# AI Twitter æœç´¢å…³é”®è¯é…ç½®
AI_TWITTER_KEYWORDS = {
    # AI å…¬å¸/å®éªŒå®¤
    "companies": [
        "OpenAI", "Anthropic", "Claude", "DeepMind", "Google AI",
        "Meta AI", "Mistral", "Cohere", "Perplexity", "xAI"
    ],
    # AI æ¨¡å‹/äº§å“
    "models": [
        "GPT-4o", "GPT-5", "Claude 4", "Gemini", "Llama 3",
        "Mistral Large", "DALL-E", "Sora", "Midjourney", "Stable Diffusion"
    ],
    # AI å¼€å‘å·¥å…·
    "dev_tools": [
        "Claude Code", "Claude Cowork", "Cursor", "GitHub Copilot", "Windsurf",
        "v0", "Replit Agent", "Devin", "LangChain", "LlamaIndex"
    ],
    # AI æŠ€æœ¯/æ¦‚å¿µ
    "technologies": [
        "AI agent", "LLM", "RAG", "fine-tuning", "multimodal",
        "AGI", "AI safety", "RLHF", "MoE", "context window"
    ],
    # AI é¢†åŸŸKOL (ç”¨äºæå‡æœç´¢è´¨é‡)
    "influencers": [
        "@sama", "@ylecun", "@kaborevsky", "@emaborevsky",
        "@AnthropicAI", "@OpenAI", "@GoogleDeepMind"
    ],
    # çªå‘æ–°é—»/äº§å“å‘å¸ƒ (æ•æ‰æœ€æ–°åŠ¨æ€)
    "breaking_news": [
        "Anthropic launches", "OpenAI announces", "Google AI releases",
        "new AI tool", "AI product launch", "just released", "now available"
    ]
}


class TechDigestAgent:
    """æŠ€æœ¯æ—¥æŠ¥ Agent - ä½¿ç”¨ Claude API ç”ŸæˆæŠ€æœ¯æ—¥æŠ¥"""

    def __init__(self):
        self.client = anthropic.Anthropic(
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
        self.model = "claude-sonnet-4-5-20250929"  # ä½¿ç”¨ Sonnet 4.5 ç”Ÿæˆå†…å®¹
        self.haiku_model = "claude-haiku-4-5-20251001"  # ä½¿ç”¨ Haiku 4.5 è¿›è¡Œæ•°æ®æ”¶é›†ï¼ˆæˆæœ¬ä½ 90%ï¼‰
        self.today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        self.today_short = datetime.now().strftime("%Y-%m-%d")
        self.output_dir = Path("output")

    def load_recent_topics(self, days: int = 7) -> str:
        """åŠ è½½æœ€è¿‘å‡ å¤©æŠ¥é“è¿‡çš„è¯é¢˜ï¼Œç”¨äºå†…å®¹å»é‡"""
        recent_topics = []

        for i in range(1, days + 1):
            date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            md_path = self.output_dir / f"tech_digest_{date}.md"

            if md_path.exists():
                try:
                    content = md_path.read_text(encoding="utf-8")
                    # æå–æ ‡é¢˜
                    title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
                    title = title_match.group(1) if title_match else ""
                    # æå–ä»Šæ—¥å¤´æ¡
                    headline_match = re.search(r'ä»Šæ—¥å¤´æ¡[ï¼š:]\s*(.+)', content)
                    headline = headline_match.group(1) if headline_match else ""
                    # æå–ç¡…è°·é›·è¾¾éƒ¨åˆ†çš„å°æ ‡é¢˜
                    radar_titles = re.findall(r'###\s*[ğŸ”¥âš ï¸ğŸ’€]\s*(.+)', content)

                    if title or headline or radar_titles:
                        topics = [f"- æ—¥æœŸ: {date}"]
                        if title:
                            topics.append(f"  æ ‡é¢˜: {title}")
                        if headline:
                            topics.append(f"  å¤´æ¡: {headline}")
                        if radar_titles:
                            topics.append(f"  é›·è¾¾: {', '.join(radar_titles[:3])}")
                        recent_topics.append("\n".join(topics))
                except Exception as e:
                    logger.warning(f"è¯»å–å†å²æ–‡ä»¶ {md_path} å¤±è´¥: {e}")

        return "\n\n".join(recent_topics) if recent_topics else ""
        
    def search_web(self, query: str, use_haiku: bool = True) -> str:
        """ä½¿ç”¨ Claude çš„ web_search å·¥å…·æœç´¢ç½‘é¡µ

        Args:
            query: æœç´¢æŸ¥è¯¢
            use_haiku: æ˜¯å¦ä½¿ç”¨ Haiku æ¨¡å‹ï¼ˆæˆæœ¬æ›´ä½ï¼‰ï¼Œé»˜è®¤ True
        """
        model = self.haiku_model if use_haiku else self.model
        logger.info(f"æœç´¢ ({model}): {query}")

        response = self.client.messages.create(
            model=model,
            max_tokens=4096,
            tools=[{
                "type": "web_search_20250305",
                "name": "web_search"
            }],
            messages=[{
                "role": "user",
                "content": f"æœç´¢ä»¥ä¸‹å†…å®¹å¹¶è¿”å›ç»“æœæ‘˜è¦ï¼š{query}"
            }]
        )

        # æå–æ–‡æœ¬å“åº”
        result = ""
        for block in response.content:
            if hasattr(block, 'text'):
                result += block.text
        return result
    
    def fetch_hn_data(self) -> str:
        """è·å– Hacker News çƒ­é—¨æ–‡ç« """
        logger.info("è·å– Hacker News æ•°æ®...")
        return self.search_web(f"Hacker News top stories today {self.today_short} site:news.ycombinator.com OR site:hntoplinks.com")
    
    def fetch_producthunt_data(self) -> str:
        """è·å– Product Hunt çƒ­é—¨äº§å“"""
        logger.info("è·å– Product Hunt æ•°æ®...")
        return self.search_web(f"Product Hunt top products {self.today_short} site:producthunt.com")
    
    def fetch_ai_twitter_data(self) -> str:
        """è·å– AI Twitter åŠ¨æ€ - å¤šç»´åº¦æœç´¢ç­–ç•¥ï¼Œåªè·å–æœ€è¿‘2å¤©çš„å†…å®¹ï¼ˆæˆæœ¬ä¼˜åŒ–ç‰ˆï¼‰"""
        logger.info("è·å– AI Twitter æ•°æ®...")

        # æ—¶é—´é™åˆ¶ï¼šåªè·å–æœ€è¿‘2å¤©çš„å†…å®¹ï¼ˆæ›´æ–°é²œï¼‰
        two_days_ago = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")
        time_filter = f"after:{two_days_ago}"
        site_filter = "site:x.com OR site:twitter.com"

        # å¤šç»´åº¦æœç´¢æŸ¥è¯¢ï¼ˆä» 5 ç»´ä¼˜åŒ–åˆ° 3 ç»´ï¼Œé™ä½æˆæœ¬ï¼‰
        search_dimensions = [
            # ç»´åº¦1: çªå‘æ–°é—»/äº§å“å‘å¸ƒ (ä¼˜å…ˆçº§æœ€é«˜ï¼Œæ•æ‰æœ€æ–°åŠ¨æ€)
            {
                "name": "AIçªå‘æ–°é—»",
                "keywords": AI_TWITTER_KEYWORDS["breaking_news"],
            },
            # ç»´åº¦2: AIå…¬å¸åŠ¨æ€ + æ¨¡å‹äº§å“ï¼ˆåˆå¹¶ï¼‰
            {
                "name": "AIå…¬å¸ä¸æ¨¡å‹",
                "keywords": AI_TWITTER_KEYWORDS["companies"][:5] + AI_TWITTER_KEYWORDS["models"][:3],
            },
            # ç»´åº¦3: AIå¼€å‘å·¥å…·ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰
            {
                "name": "AIå¼€å‘å·¥å…·",
                "keywords": AI_TWITTER_KEYWORDS["dev_tools"][:6],  # åŒ…å« Claude Code å’Œ Cowork
            },
        ]

        all_results = []
        for dimension in search_dimensions:
            keywords_str = " OR ".join(dimension["keywords"])
            query = f"({keywords_str}) latest news {time_filter} {site_filter}"
            logger.info(f"æœç´¢ {dimension['name']}: {query[:80]}...")

            result = self.search_web(query)
            if result:
                all_results.append(f"### {dimension['name']}\n{result}")

        return "\n\n".join(all_results) if all_results else "æœªè·å–åˆ°AI Twitteræ•°æ®"

    def fetch_reddit_ml_data(self) -> str:
        """è·å– Reddit r/MachineLearning çƒ­é—¨å¸–å­"""
        logger.info("è·å– Reddit ML æ•°æ®...")
        two_days_ago = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")
        return self.search_web(f"site:reddit.com/r/MachineLearning top posts after:{two_days_ago}")

    def fetch_github_trending(self) -> str:
        """è·å– GitHub Trending é¡¹ç›®"""
        logger.info("è·å– GitHub Trending æ•°æ®...")
        return self.search_web(f"GitHub trending repositories {self.today_short} AI machine learning site:github.com/trending OR site:github.blog")

    def generate_digest(self, hn_data: str, ph_data: str, twitter_data: str,
                        reddit_data: str = "", github_data: str = "") -> Dict[str, str]:
        """ä½¿ç”¨ Claude ç”ŸæˆæŠ€æœ¯æ—¥æŠ¥ï¼ˆåˆ†ä¸¤æ­¥ï¼šå…ˆ Markdownï¼Œå HTMLï¼‰"""
        logger.info("ç”ŸæˆæŠ€æœ¯æ—¥æŠ¥ Markdown...")

        # åŠ è½½å†å²è¯é¢˜ç”¨äºå»é‡
        recent_topics = self.load_recent_topics(days=7)
        dedup_instruction = ""
        if recent_topics:
            dedup_instruction = f"""
## é‡è¦ï¼šå†…å®¹å»é‡è¦æ±‚
ä»¥ä¸‹æ˜¯è¿‡å»7å¤©å·²ç»æŠ¥é“è¿‡çš„è¯é¢˜ï¼Œè¯·**é¿å…é‡å¤æŠ¥é“ç›¸åŒå†…å®¹**ï¼š
- å¦‚æœæŸä¸ªè¯é¢˜å·²ç»æŠ¥é“è¿‡ï¼Œé™¤éæœ‰é‡å¤§è¿›å±•ï¼Œå¦åˆ™ä¸è¦å†ä½œä¸ºå¤´æ¡æˆ–ä¸»è¦å†…å®¹
- å¦‚æœå¿…é¡»æåŠå·²æŠ¥é“çš„è¯é¢˜ï¼Œè¯·ç”¨æ–°çš„è§’åº¦ã€æ–°çš„è§‚ç‚¹æ¥è§£è¯»
- ä¼˜å…ˆé€‰æ‹©ä»Šå¤©æ•°æ®æºä¸­çš„æ–°è¯é¢˜

### å·²æŠ¥é“è¯é¢˜åˆ—è¡¨ï¼š
{recent_topics}

"""

        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆ Markdown
        markdown_prompt = f"""ä½ æ˜¯ä¸€ä½åœ¨ç¡…è°·å·¥ä½œå¤šå¹´çš„åäººæŠ€æœ¯è€å…µï¼ŒåŒæ—¶è¿è¥ä¸€ä¸ªå°ä¼—ä½†æœ‰æ·±åº¦çš„æŠ€æœ¯å…¬ä¼—å·ã€ŒTechè€å…µæ—¥è®°ã€ã€‚ä½ çš„é£æ ¼æ˜¯ï¼š
- è¯´è¯ç›´æ¥ï¼Œå¶å°”æ¯’èˆŒï¼Œä½†è§‚ç‚¹çŠ€åˆ©
- å–œæ¬¢ç”¨ç±»æ¯”å’Œæ¯”å–»è§£é‡Šå¤æ‚æ¦‚å¿µ
- ä¼šåŠ å…¥è‡ªå·±çš„åˆ¤æ–­å’Œé¢„æµ‹ï¼Œæ•¢äºè¡¨æ€ï¼ˆæ¯”å¦‚"è¿™ä¸ªæˆ‘ä¸çœ‹å¥½"ã€"è¿™ä¸ªå€¼å¾—å…³æ³¨"ï¼‰
- å¶å°”åæ§½è¡Œä¸šä¹±è±¡æˆ–è¿‡åº¦ç‚’ä½œ
- è¯­æ°”åƒè·Ÿæœ‹å‹èŠå¤©ï¼Œä¸æ˜¯å†™æŠ¥å‘Š
- ä¼šåˆ†äº«ä¸€äº›"åœˆå†…äººæ‰çŸ¥é“"çš„æ´å¯Ÿ

è¯·æ ¹æ®ä»¥ä¸‹æ•°æ®æºï¼Œç”¨ä½ çš„é£æ ¼å†™ä¸€ä»½ Markdown æ ¼å¼çš„æŠ€æœ¯æ—¥æŠ¥ã€‚

## ğŸš¨ æå…¶é‡è¦ï¼šå†…å®¹æ–°é²œåº¦è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼‰
- **ä»Šå¤©æ—¥æœŸæ˜¯ {self.today}**
- **åªä½¿ç”¨æœ€è¿‘ 24-48 å°æ—¶å†…çš„æ–°é—»å’Œä¿¡æ¯**
- **ä¸¥æ ¼è¿‡æ»¤æ‰è¶…è¿‡ 2 å¤©çš„æ—§æ–°é—»**ï¼ˆä»»ä½•è¶…è¿‡ 48 å°æ—¶çš„å†…å®¹éƒ½è§†ä¸ºè¿‡æ—¶ï¼‰
- å¦‚æœæ•°æ®æºä¸­åŒ…å«æ—§ä¿¡æ¯ï¼ˆå¦‚å»å¹´çš„äº§å“å‘å¸ƒã€ä¸Šå‘¨çš„æ–°é—»ç­‰ï¼‰ï¼Œç›´æ¥å¿½ç•¥è¿™äº›å†…å®¹
- **ä¼˜å…ˆçº§ï¼šä»Šå¤© > æ˜¨å¤© > å‰å¤©**ï¼Œè¶Šæ–°é²œçš„å†…å®¹è¶Šé‡è¦
- å®å¯å†…å®¹å°‘ä¸€äº›ï¼Œä¹Ÿä¸è¦ä½¿ç”¨è¿‡æ—¶çš„ä¿¡æ¯è¯¯å¯¼è¯»è€…
{dedup_instruction}
## æ•°æ®æº

### Hacker News
{hn_data}

### Product Hunt
{ph_data}

### AI Twitter
{twitter_data}

### Reddit r/MachineLearning
{reddit_data if reddit_data else "æš‚æ— æ•°æ®"}

### GitHub Trending
{github_data if github_data else "æš‚æ— æ•°æ®"}

## è¾“å‡ºè¦æ±‚

### é‡è¦ï¼šæ–‡ç« é•¿åº¦å’ŒSEOä¼˜åŒ–
- **æ–‡ç« æ€»é•¿åº¦å¿…é¡»åœ¨ 1500-2500 å­—ä¹‹é—´**ï¼Œå†…å®¹è¦å……å®æœ‰æ–™
- **æ ‡é¢˜è¦åŒ…å«çƒ­é—¨å…³é”®è¯**ï¼ˆå¦‚ï¼šAIã€Claudeã€GPTã€æ•ˆç‡å·¥å…·ã€ç¨‹åºå‘˜ ç­‰ï¼‰ï¼Œå¸å¼•æœç´¢æµé‡
- **æ­£æ–‡è‡ªç„¶èå…¥é•¿å°¾å…³é”®è¯**ï¼ˆå¦‚ï¼šAIå·¥å…·æ¨èã€ç¨‹åºå‘˜æ•ˆç‡ã€ç§‘æŠ€è¶‹åŠ¿ã€ç¡…è°·è§é—» ç­‰ï¼‰ï¼Œæ¯300å­—å‡ºç°2-3æ¬¡
- **è®¾ç½®äº’åŠ¨é’©å­**ï¼šåœ¨æ–‡ä¸­å’Œæ–‡æœ«å¼•å¯¼è¯»è€…ç‚¹èµã€è¯„è®º
- **å¼•å¯¼å…³æ³¨**ï¼šåœ¨åˆé€‚ä½ç½®è‡ªç„¶åœ°æåŠå…³æ³¨å…¬ä¼—å·çš„å¥½å¤„

### Markdown ç»“æ„è¦æ±‚
æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆï¼Œæ³¨æ„ä¿æŒä¸ªäººé£æ ¼å’Œå……å®å†…å®¹ï¼š

1. **å¼€ç¯‡å¼•è¨€**ï¼ˆ2-3å¥å¼•äººå…¥èƒœçš„è¯ï¼Œåˆ¶é€ æ‚¬å¿µæˆ–æŠ›å‡ºè§‚ç‚¹ï¼Œè®©è¯»è€…æƒ³ç»§ç»­çœ‹ä¸‹å»ï¼‰

2. **ä»Šæ—¥å¤´æ¡ï¼šXXX**ï¼ˆé’ˆå¯¹ä»Šå¤©æœ€é‡è¦çš„1ä¸ªäº‹ä»¶ï¼Œæ·±åº¦åˆ†æ 300-400å­—ï¼‰
   - è¿™æ˜¯ä»€ä¹ˆï¼šç”¨å¤§ç™½è¯è§£é‡Š
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šå¯¹è¡Œä¸š/å¼€å‘è€…çš„å½±å“
   - æˆ‘çš„çœ‹æ³•ï¼šä¸ªäººåˆ¤æ–­å’Œé¢„æµ‹
   - ä½ åº”è¯¥å…³æ³¨çš„ç‚¹ï¼šå…·ä½“å»ºè®®

3. **ç¡…è°·é›·è¾¾ï¼šæœ¬å‘¨å€¼å¾—å…³æ³¨**ï¼ˆ2-3ä¸ªé‡è¦è¶‹åŠ¿ï¼Œæ¯ä¸ª150-200å­—ï¼‰
   - ç”¨ ğŸ”¥ æ ‡è®°å¼ºçƒˆçœ‹å¥½ï¼Œâš ï¸ æ ‡è®°éœ€è¦è§‚æœ›ï¼ŒğŸ’€ æ ‡è®°ä¸çœ‹å¥½
   - æ¯ä¸ªéƒ½è¦æœ‰"è¿™æ„å‘³ç€ä»€ä¹ˆ"çš„åˆ†æ

4. **HN çƒ­æ¦œç²¾é€‰**ï¼ˆ8-10ä¸ªé¡¹ç›®ï¼‰
   - **Markdown æ ¼å¼**ï¼šä½¿ç”¨ç®€åŒ–è¡¨æ ¼ï¼ˆ3åˆ—ï¼‰æˆ–ç¼–å·åˆ—è¡¨
     ```
     | æ’å | æ ‡é¢˜ (çƒ­åº¦) | ä¸ºä»€ä¹ˆå€¼å¾—çœ‹ |
     æˆ–è€…
     1. **æ ‡é¢˜** (çƒ­åº¦) - ä¸ºä»€ä¹ˆå€¼å¾—çœ‹
     ```
   - æŒ‘2-3ä¸ªç‰¹åˆ«æœ‰æ„æ€çš„ï¼Œåœ¨è¡¨æ ¼/åˆ—è¡¨åé¢å¤–å†™å‡ å¥æ·±åº¦ç‚¹è¯„

5. **Product Hunt ä»Šæ—¥å‘ç°**ï¼ˆ4-5ä¸ªäº§å“ï¼‰
   - **Markdown æ ¼å¼**ï¼šä½¿ç”¨ç®€å•åˆ—è¡¨
     ```
     **äº§å“å** - ä¸€å¥è¯ä»‹ç»
     äº®ç‚¹ï¼šxxx | æé†’ï¼šxxx
     ```
   - å¯¹ç‰¹åˆ«æœ‰æ„æ€çš„äº§å“ï¼Œè¡¥å……"è¿™ä¸ªäº§å“è§£å†³äº†ä»€ä¹ˆç—›ç‚¹"çš„åˆ†æ

6. **GitHub Trending æœ¬å‘¨çƒ­é—¨**ï¼ˆ**å¿…é¡»åŒ…å« 3-5 ä¸ªå¼€æºé¡¹ç›®ï¼Œä¸èƒ½å°‘äº 3 ä¸ª**ï¼‰
   - **Markdown æ ¼å¼**ï¼šä½¿ç”¨ç®€å•åˆ—è¡¨ï¼Œæ¯ä¸ªé¡¹ç›®ä¸€è¡Œ
     ```
     1. **é¡¹ç›®å** (è¯­è¨€ Â· â­Staræ•°) - ä¸€å¥è¯æè¿°
     2. **é¡¹ç›®å** (è¯­è¨€ Â· â­Staræ•°) - ä¸€å¥è¯æè¿°
     ```
   - **å¼ºåˆ¶è¦æ±‚ï¼šå¿…é¡»åˆ—å‡ºè‡³å°‘ 3 ä¸ªé¡¹ç›®ï¼Œæœ€å¤š 5 ä¸ª**
   - ä¼˜å…ˆé€‰æ‹© AI/ML ç›¸å…³çš„é¡¹ç›®
   - ç®€è¦è¯´æ˜é¡¹ç›®è§£å†³ä»€ä¹ˆé—®é¢˜ã€é€‚åˆè°ç”¨
   - å¦‚æœæ•°æ®æºä¸­æ²¡æœ‰è¶³å¤Ÿé¡¹ç›®ï¼Œå¯ä»¥è¡¥å……"å€¼å¾—å…³æ³¨çš„ç»å…¸é¡¹ç›®"

7. **AI åœˆå†…å¹•**ï¼ˆè¿™éƒ¨åˆ†è¦å†™è¯¦ç»†ï¼Œ400-500å­—ï¼‰
   - å¤§å‚åŠ¨æ€ï¼šè°å‘å¸ƒäº†ä»€ä¹ˆï¼Œæ„å‘³ç€ä»€ä¹ˆ
   - å¼€æºç¤¾åŒºï¼šæœ‰ä»€ä¹ˆæ–°é¡¹ç›®å€¼å¾—å…³æ³¨ï¼ˆç»“åˆ GitHub Trending å’Œ Reddit è®¨è®ºï¼‰
   - å·¥å…·æ¨èï¼šæˆ‘æœ€è¿‘åœ¨ç”¨ä»€ä¹ˆï¼Œä½“éªŒå¦‚ä½•
   - è¡Œä¸šå…«å¦ï¼šæœ‰ä»€ä¹ˆæœ‰æ„æ€çš„äº‹æƒ…ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰

8. **æœ¬å‘¨å®æ“å»ºè®®**ï¼ˆ2-3ä¸ªå…·ä½“å¯è½åœ°çš„è¡ŒåŠ¨é¡¹ï¼‰
   - ä¸è¦å‡å¤§ç©ºï¼Œè¦å…·ä½“åˆ°"æ‰“å¼€xxxï¼Œè¯•è¯•xxxåŠŸèƒ½"
   - å¯ä»¥æ˜¯å·¥å…·æ¨èã€å­¦ä¹ èµ„æºã€æˆ–è€…æ€ç»´æ–¹å¼

9. **è€å…µç¢ç¢å¿µ**ï¼ˆ150-200å­—çš„ä¸ªäººæ„Ÿæ‚Ÿæˆ–æ€è€ƒï¼‰
   - å¯ä»¥æ˜¯å¯¹è¡Œä¸šçš„æ€è€ƒã€èŒä¸šå»ºè®®ã€æˆ–è€…ç”Ÿæ´»æ„Ÿæ‚Ÿ
   - è¯­æ°”è¦çœŸè¯šï¼Œåƒè·Ÿè€æœ‹å‹èŠå¤©

10. **äº’åŠ¨æ—¶é—´**
    - æŠ›å‡º1-2ä¸ªé—®é¢˜ï¼Œå¼•å¯¼è¯»è€…åœ¨è¯„è®ºåŒºè®¨è®º
    - ä¾‹å¦‚ï¼š"ä½ è§‰å¾—xxxæ€ä¹ˆæ ·ï¼Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºèŠèŠ"
    - åŠ ä¸€å¥"è§‰å¾—æœ‰ç”¨çš„è¯ï¼Œç‚¹ä¸ªèµæ”¯æŒä¸€ä¸‹ğŸ‘‡"

11. **ä¸‹æœŸé¢„å‘Š**ï¼ˆ1-2å¥è¯ï¼Œåˆ¶é€ æœŸå¾…æ„Ÿï¼‰
    - é¢„å‘Šä¸‹ä¸€æœŸå¯èƒ½ä¼šèŠçš„è¯é¢˜
    - å¼•å¯¼å…³æ³¨ï¼š"å…³æ³¨å…¬ä¼—å·ï¼Œç¬¬ä¸€æ—¶é—´è·å–æ›´æ–°"

ç›´æ¥è¾“å‡ºå®Œæ•´çš„ Markdown å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ ‡ç­¾æˆ–å‰ç¼€ã€‚
"""

        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆ Markdown
        response = self.client.messages.create(
            model=self.model,
            max_tokens=16384,
            messages=[{
                "role": "user",
                "content": markdown_prompt
            }]
        )

        # æå– Markdown æ–‡æœ¬
        markdown_content = ""
        for block in response.content:
            if hasattr(block, 'text'):
                markdown_content += block.text

        markdown_content = markdown_content.strip()
        logger.info(f"Markdown ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(markdown_content)} å­—ç¬¦")

        # ç¬¬äºŒæ­¥ï¼šåŸºäº Markdown ç”Ÿæˆ HTML
        logger.info("ç”Ÿæˆå¾®ä¿¡å…¬ä¼—å· HTML...")
        html_prompt = f"""è¯·å°†ä»¥ä¸‹ Markdown æŠ€æœ¯æ—¥æŠ¥è½¬æ¢ä¸ºé€‚é…å¾®ä¿¡å…¬ä¼—å·çš„å¯Œæ–‡æœ¬ HTMLã€‚

## åŸå§‹ Markdown å†…å®¹ï¼š
{markdown_content}

## HTML æ ·å¼è§„èŒƒï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š

**å¸ƒå±€è§„èŒƒï¼š**
- å¡ç‰‡å®¹å™¨ï¼špadding: 15px; margin-bottom: 15px;
- æ ‡é¢˜å’Œå†…å®¹ä¹‹é—´ï¼šmargin-bottom: 10px;
- è¡¨æ ¼ï¼šmargin-top: 10px; width: 100%;
- ç¦æ­¢ä½¿ç”¨ min-height æˆ–å›ºå®š height
- **åˆ—è¡¨æ ‡ç­¾å¿…é¡»ç´§å‡‘æ’åˆ—ï¼** æ­£ç¡®æ ¼å¼ï¼š`<ul><li>å†…å®¹1</li><li>å†…å®¹2</li></ul>`

**é¢œè‰²è§„èŒƒï¼ˆæå…¶é‡è¦ï¼ï¼‰ï¼š**
- **æ‰€æœ‰é¢œè‰²å¿…é¡»ä½¿ç”¨åå…­è¿›åˆ¶æ ¼å¼**ï¼šç™½è‰² #ffffffï¼Œé»‘è‰² #000000
- ç¦æ­¢ä½¿ç”¨ whiteã€blackã€red ç­‰é¢œè‰²åç§°ï¼

**æ ·å¼è§„èŒƒï¼š**
- ä½¿ç”¨å†…è”æ ·å¼
- å¡ç‰‡èƒŒæ™¯ï¼šlinear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%)
- å¡ç‰‡åœ†è§’ï¼šborder-radius: 12px;
- **è¡¨æ ¼è¡¨å¤´**ï¼šbackground: #667eea; color: #ffffff;
- è¡¨æ ¼æ•°æ®è¡Œï¼šäº¤æ›¿èƒŒæ™¯è‰² #ffffff å’Œ #f8f9fa
- å­—ä½“å¤§å°ï¼šæ ‡é¢˜ 16-18pxï¼Œæ­£æ–‡ 14-15pxï¼Œè¡¨æ ¼ 13px
- è¡Œé—´è·ï¼šline-height: 1.8;

**ç‰¹æ®ŠåŒºå—æ ·å¼ï¼š**

é˜…è¯»æ—¶é—´æç¤ºï¼ˆæ”¾åœ¨æœ€å¼€å¤´ï¼‰ï¼š
<div style="text-align: center; color: #888888; font-size: 13px; padding: 10px 0; margin-bottom: 15px; border-bottom: 1px dashed #e0e0e0;">
  æœ¬æ–‡å…±çº¦{len(markdown_content)}å­— | é¢„è®¡é˜…è¯»æ—¶é—´{len(markdown_content)//300}åˆ†é’Ÿ
</div>

ä»Šæ—¥å¤´æ¡åŒºå—ï¼š
<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 12px; margin: 20px 0;">
  <h2 style="color: #ffffff; font-size: 18px; margin: 0 0 15px 0;">ğŸ“Œ ä»Šæ—¥å¤´æ¡ï¼šXXX</h2>
  <p style="color: #ffffff; font-size: 14px; margin-bottom: 10px;">å†…å®¹...</p>
</div>

äº’åŠ¨å¼•å¯¼ï¼š
<div style="background: linear-gradient(135deg, #ff6b6b 0%, #feca57 100%); padding: 20px; border-radius: 12px; text-align: center; margin: 20px 0;">
  <p style="color: #ffffff; font-size: 16px; font-weight: bold; margin: 0;">ğŸ‘‡ è§‰å¾—æœ‰ç”¨ï¼Ÿç‚¹ä¸ªèµæ”¯æŒä¸€ä¸‹</p>
</div>

**ğŸ“± ç§»åŠ¨ç«¯å¸ƒå±€ï¼ˆæå…¶é‡è¦ï¼‰ï¼š**
- GitHub Trending / Product Hunt å¿…é¡»ä½¿ç”¨å¡ç‰‡å¼å¸ƒå±€
- HN çƒ­æ¦œå¯ä»¥ä½¿ç”¨ç®€åŒ–è¡¨æ ¼ï¼ˆæœ€å¤š3åˆ—ï¼‰

GitHub Trending å¡ç‰‡ç¤ºä¾‹ï¼š
<div style="background: linear-gradient(135deg, #f5f7fa 0%, #e3e8f0 100%); padding: 15px; border-radius: 12px; margin-bottom: 12px; border-left: 4px solid #667eea;">
  <div style="margin-bottom: 8px;">
    <strong style="color: #333333; font-size: 15px;">é¡¹ç›®å</strong>
    <span style="background: #667eea; color: #ffffff; padding: 3px 8px; border-radius: 10px; font-size: 12px; margin-left: 8px;">â­ 15K+</span>
  </div>
  <div style="color: #888888; font-size: 12px; margin-bottom: 8px;">ğŸ Python</div>
  <p style="margin: 0; color: #555555; font-size: 14px; line-height: 1.6;">é¡¹ç›®æè¿°</p>
</div>

**è¾“å‡ºè¦æ±‚ï¼š**
ç›´æ¥è¾“å‡ºå®Œæ•´çš„ HTML ä»£ç ï¼Œä»¥ <div> å¼€å§‹ï¼Œä¸è¦åŒ…å« <!DOCTYPE>ã€<html>ã€<head>ã€<body> ç­‰æ ‡ç­¾ã€‚
ä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æ–‡å­—ï¼Œåªè¾“å‡º HTML ä»£ç ã€‚
"""

        html_response = self.client.messages.create(
            model=self.model,
            max_tokens=16384,
            messages=[{
                "role": "user",
                "content": html_prompt
            }]
        )

        # æå– HTML æ–‡æœ¬
        html_content = ""
        for block in html_response.content:
            if hasattr(block, 'text'):
                html_content += block.text

        html_content = html_content.strip()

        # æ¸…ç† HTML ä»£ç å—æ ‡è®°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if html_content.startswith("```html"):
            html_content = html_content[7:]
        if html_content.startswith("```"):
            html_content = html_content[3:]
        if html_content.endswith("```"):
            html_content = html_content[:-3]
        html_content = html_content.strip()

        # æ¸…ç†å’Œä¿®å¤ HTML ä»¥é€‚é…å¾®ä¿¡å…¬ä¼—å·
        if html_content:
            html_content = self._clean_html_for_wechat(html_content)
            logger.info(f"HTML ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
        else:
            logger.warning("HTML å†…å®¹ä¸ºç©º")

        return {
            "markdown": markdown_content,
            "html": html_content,
            "raw": markdown_content
        }

    def _clean_html_for_wechat(self, html: str) -> str:
        """æ¸…ç†å’Œä¿®å¤ HTML ä»¥é€‚é…å¾®ä¿¡å…¬ä¼—å·çš„æ¸²æŸ“"""
        # 1. æ¸…ç†åˆ—è¡¨æ ‡ç­¾ä¹‹é—´çš„ç©ºç™½ï¼ˆé¿å…å¾®ä¿¡æ˜¾ç¤ºç©ºåˆ—è¡¨é¡¹ï¼‰
        html = re.sub(r'<ul([^>]*)>\s+<li', r'<ul\1><li', html)
        html = re.sub(r'</li>\s+<li', r'</li><li', html)
        html = re.sub(r'</li>\s+</ul>', r'</li></ul>', html)
        html = re.sub(r'<ol([^>]*)>\s+<li', r'<ol\1><li', html)
        html = re.sub(r'</li>\s+</ol>', r'</li></ol>', html)

        # 2. ä¿®å¤è¡¨æ ¼è¡¨å¤´ï¼šç¡®ä¿æ¯ä¸ª th éƒ½æœ‰èƒŒæ™¯è‰²ï¼ˆå¾®ä¿¡ä¸æ”¯æŒåœ¨ tr ä¸Šè®¾ç½®èƒŒæ™¯ï¼‰
        def fix_th_background(match):
            th_tag = match.group(0)
            # å¦‚æœ th å·²ç»æœ‰ background æ ·å¼ï¼Œè·³è¿‡
            if 'background' in th_tag.lower():
                return th_tag
            # åœ¨ style ä¸­æ·»åŠ  background
            if 'style="' in th_tag:
                return th_tag.replace('style="', 'style="background: #667eea; ')
            else:
                return th_tag.replace('<th', '<th style="background: #667eea;"')

        # ä½¿ç”¨è´Ÿå‘å‰ç» (?!ead) æ¥é¿å…åŒ¹é… <thead>
        html = re.sub(r'<th(?!ead)([^>]*)>', fix_th_background, html)

        # 3. æ¸…ç†è¡¨æ ¼æ ‡ç­¾ä¹‹é—´çš„ç©ºç™½
        html = re.sub(r'<table([^>]*)>\s+<thead', r'<table\1><thead', html)
        html = re.sub(r'<thead>\s+<tr', r'<thead><tr', html)
        html = re.sub(r'<tr([^>]*)>\s+<th', r'<tr\1><th', html)
        html = re.sub(r'</th>\s+<th', r'</th><th', html)
        html = re.sub(r'</th>\s+</tr>', r'</th></tr>', html)
        html = re.sub(r'</tr>\s+</thead>', r'</tr></thead>', html)
        html = re.sub(r'</thead>\s+<tbody>', r'</thead><tbody>', html)
        html = re.sub(r'<tbody>\s+<tr', r'<tbody><tr', html)
        html = re.sub(r'</tr>\s+<tr', r'</tr><tr', html)
        html = re.sub(r'</tr>\s+</tbody>', r'</tr></tbody>', html)
        html = re.sub(r'</tbody>\s+</table>', r'</tbody></table>', html)

        return html
    
    def run(self, max_retries: int = 3) -> Dict[str, str]:
        """æ‰§è¡Œå®Œæ•´çš„æ—¥æŠ¥ç”Ÿæˆæµç¨‹"""
        logger.info(f"=== å¼€å§‹ç”Ÿæˆ {self.today} æŠ€æœ¯æ—¥æŠ¥ ===")

        # è·å–æ•°æ®æº
        hn_data = self.fetch_hn_data()
        ph_data = self.fetch_producthunt_data()
        twitter_data = self.fetch_ai_twitter_data()
        reddit_data = self.fetch_reddit_ml_data()
        github_data = self.fetch_github_trending()

        # ç”Ÿæˆæ—¥æŠ¥ï¼Œå¸¦é‡è¯•é€»è¾‘ç¡®ä¿ HTML è¾“å‡º
        digest = {}
        for attempt in range(1, max_retries + 1):
            try:
                digest = self.generate_digest(hn_data, ph_data, twitter_data, reddit_data, github_data)
                # æ£€æŸ¥ HTML æ˜¯å¦æœ‰æ•ˆï¼ˆä¸ä¸ºç©ºä¸”é•¿åº¦åˆç†ï¼‰
                if digest.get("html") and len(digest["html"]) > 500:
                    logger.info(f"æ—¥æŠ¥ç”ŸæˆæˆåŠŸ (ç¬¬ {attempt} æ¬¡å°è¯•)")
                    break
                else:
                    html_len = len(digest.get("html", ""))
                    logger.warning(f"ç¬¬ {attempt} æ¬¡ç”Ÿæˆ HTML ä¸åˆæ ¼ï¼ˆé•¿åº¦: {html_len}ï¼‰ï¼Œé‡è¯•ä¸­...")
            except Exception as e:
                logger.error(f"ç¬¬ {attempt} æ¬¡ç”Ÿæˆå‡ºé”™: {str(e)}")
                if attempt >= max_retries:
                    raise

        if not digest.get("html") or len(digest["html"]) < 500:
            logger.error("æ‰€æœ‰å°è¯•å‡æœªç”Ÿæˆæœ‰æ•ˆ HTML å†…å®¹ï¼Œæ— æ³•å‘å¸ƒåˆ°å…¬ä¼—å·")
        
        # ä¿å­˜æ–‡ä»¶
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        md_path = output_dir / f"tech_digest_{self.today_short}.md"
        html_path = output_dir / f"tech_digest_{self.today_short}.html"
        
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(digest["markdown"])
        logger.info(f"Markdown å·²ä¿å­˜: {md_path}")
        
        if digest["html"]:
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(digest["html"])
            logger.info(f"HTML å·²ä¿å­˜: {html_path}")
        
        digest["md_path"] = str(md_path)
        digest["html_path"] = str(html_path) if digest["html"] else None
        
        return digest


class WeChatPublisher:
    """å¾®ä¿¡å…¬ä¼—å·å‘å¸ƒå™¨"""
    
    def __init__(self):
        self.app_id = os.getenv("WECHAT_APP_ID")
        self.app_secret = os.getenv("WECHAT_APP_SECRET")
        self.access_token = None
        
    def get_access_token(self) -> str:
        """è·å– access_token"""
        if self.access_token:
            return self.access_token
            
        url = f"https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&appid={self.app_id}&secret={self.app_secret}"
        resp = requests.get(url, timeout=10)
        result = resp.json()
        
        if "access_token" in result:
            self.access_token = result["access_token"]
            logger.info("access_token è·å–æˆåŠŸ")
            return self.access_token
        else:
            raise Exception(f"è·å– access_token å¤±è´¥: {result}")
    
    def create_cover_image(self, title: str = "Tech Digest", keywords: List[str] = None) -> str:
        """ç”Ÿæˆå°é¢å›¾ç‰‡ï¼Œæ”¯æŒæ˜¾ç¤ºæ¯æ—¥å…³é”®è¯"""
        img = Image.new('RGB', (900, 383), color='#667eea')
        draw = ImageDraw.Draw(img)

        # æ¸å˜èƒŒæ™¯
        for i in range(383):
            r = int(102 + (118-102) * i / 383)
            g = int(126 + (75-126) * i / 383)
            b = int(234 + (162-234) * i / 383)
            draw.line([(0, i), (900, i)], fill=(r, g, b))

        # å­—ä½“
        try:
            font_large = ImageFont.truetype("/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc", 48)
            font_small = ImageFont.truetype("/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc", 24)
            font_tag = ImageFont.truetype("/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc", 18)
        except OSError:
            font_large = ImageFont.load_default()
            font_small = ImageFont.load_default()
            font_tag = ImageFont.load_default()

        today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        draw.text((450, 120), title, font=font_large, fill='white', anchor='mm')
        draw.text((450, 175), today, font=font_small, fill='white', anchor='mm')

        # ç»˜åˆ¶å…³é”®è¯æ ‡ç­¾
        if keywords:
            tags = keywords[:4]  # æœ€å¤šæ˜¾ç¤º4ä¸ªå…³é”®è¯
            tag_y = 230
            total_width = sum(len(tag) * 18 + 30 for tag in tags) + (len(tags) - 1) * 10
            start_x = (900 - total_width) // 2

            for tag in tags:
                tag_width = len(tag) * 18 + 20
                # ç»˜åˆ¶åœ†è§’çŸ©å½¢èƒŒæ™¯ï¼ˆæ·±è‰²åŠé€æ˜æ•ˆæœï¼‰
                draw.rounded_rectangle(
                    [start_x, tag_y, start_x + tag_width, tag_y + 30],
                    radius=15,
                    fill=(50, 50, 80)  # æ·±è“ç´«è‰²èƒŒæ™¯
                )
                # ç»˜åˆ¶æ–‡å­—ï¼ˆç™½è‰²ï¼‰
                draw.text((start_x + tag_width // 2, tag_y + 15), f"#{tag}",
                         font=font_tag, fill='#ffffff', anchor='mm')
                start_x += tag_width + 10

        cover_path = "output/cover.jpg"
        img.save(cover_path, "JPEG", quality=95)
        logger.info(f"å°é¢å›¾ç‰‡å·²ç”Ÿæˆ: {cover_path}")
        return cover_path
    
    def upload_image(self, image_path: str) -> str:
        """ä¸Šä¼ å›¾ç‰‡åˆ°å¾®ä¿¡"""
        token = self.get_access_token()
        url = f"https://api.weixin.qq.com/cgi-bin/material/add_material?access_token={token}&type=image"
        
        with open(image_path, "rb") as f:
            files = {"media": ("cover.jpg", f, "image/jpeg")}
            resp = requests.post(url, files=files, timeout=30)
            result = resp.json()
        
        if "media_id" in result:
            logger.info(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {result['media_id']}")
            return result["media_id"]
        else:
            raise Exception(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {result}")
    
    def create_draft(self, title: str, content: str, thumb_media_id: str) -> str:
        """åˆ›å»ºè‰ç¨¿"""
        token = self.get_access_token()
        url = f"https://api.weixin.qq.com/cgi-bin/draft/add?access_token={token}"
        
        article = {
            "articles": [{
                "title": title,
                "author": "Tech Digest",
                "digest": "æ¯æ—¥æŠ€æœ¯è¶‹åŠ¿ç²¾é€‰",
                "content": content,
                "thumb_media_id": thumb_media_id,
                "need_open_comment": 1,
                "only_fans_can_comment": 0
            }]
        }
        
        # å…³é”®ï¼šç¡®ä¿ä¸­æ–‡æ­£ç¡®ç¼–ç 
        json_data = json.dumps(article, ensure_ascii=False).encode('utf-8')
        resp = requests.post(
            url,
            data=json_data,
            headers={"Content-Type": "application/json; charset=utf-8"},
            timeout=30
        )
        result = resp.json()
        
        if "media_id" in result:
            logger.info(f"è‰ç¨¿åˆ›å»ºæˆåŠŸ: {result['media_id']}")
            return result["media_id"]
        else:
            raise Exception(f"è‰ç¨¿åˆ›å»ºå¤±è´¥: {result}")
    
    def publish(self, draft_media_id: str) -> Optional[str]:
        """å‘å¸ƒæ–‡ç« ï¼ˆéœ€è¦è®¤è¯å…¬ä¼—å·ï¼‰"""
        token = self.get_access_token()
        url = f"https://api.weixin.qq.com/cgi-bin/freepublish/submit?access_token={token}"
        
        resp = requests.post(url, json={"media_id": draft_media_id}, timeout=30)
        result = resp.json()
        
        if result.get("errcode") == 0:
            logger.info(f"å‘å¸ƒæˆåŠŸ: {result.get('publish_id')}")
            return result.get("publish_id")
        elif result.get("errcode") == 48001:
            logger.warning("å‘å¸ƒAPIæœªæˆæƒï¼Œè¯·æ‰‹åŠ¨åˆ°å…¬ä¼—å·åå°å‘å¸ƒè‰ç¨¿")
            return None
        else:
            raise Exception(f"å‘å¸ƒå¤±è´¥: {result}")
    
    def run(self, html_content: str, title: str = "Tech Digest", keywords: List[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´å‘å¸ƒæµç¨‹"""
        logger.info("=== å¼€å§‹å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å· ===")

        # ç”Ÿæˆå°é¢ï¼ˆå¸¦å…³é”®è¯ï¼‰
        cover_path = self.create_cover_image(title, keywords)
        
        # ä¸Šä¼ å°é¢
        thumb_media_id = self.upload_image(cover_path)
        
        # åˆ›å»ºè‰ç¨¿
        draft_media_id = self.create_draft(title, html_content, thumb_media_id)
        
        # å°è¯•å‘å¸ƒ
        publish_id = self.publish(draft_media_id)
        
        return {
            "thumb_media_id": thumb_media_id,
            "draft_media_id": draft_media_id,
            "publish_id": publish_id,
            "status": "published" if publish_id else "draft_created"
        }


def extract_keywords_from_markdown(markdown: str) -> List[str]:
    """ä» Markdown å†…å®¹ä¸­æå–å…³é”®è¯ï¼ˆç”¨äºå°é¢å›¾ï¼‰"""
    keywords = []

    # 1. ä»æ ‡é¢˜ä¸­æå–ï¼ˆæ ¼å¼å¦‚ï¼š# Techè€å…µæ—¥è®° | 2026.01.16ï¼šClaudeæ³„å¯†ã€GitHub Actionsè¢«éª‚ï¼‰
    title_match = re.search(r'^#\s+[^|]+\|[^ï¼š:]+[ï¼š:]\s*(.+)$', markdown, re.MULTILINE)
    if title_match:
        title_part = title_match.group(1)
        # æå–ä¸­æ–‡/è‹±æ–‡å…³é”®è¯çŸ­è¯­ï¼Œç”¨é¡¿å·æˆ–é€—å·åˆ†éš”
        phrases = re.split(r'[ã€ï¼Œ,]', title_part)
        for phrase in phrases[:3]:
            phrase = phrase.strip()
            if phrase and len(phrase) <= 15:
                keywords.append(phrase)

    # 2. å¦‚æœæ ‡é¢˜æ²¡æœ‰æå–åˆ°ï¼Œå°è¯•ä»ä»Šæ—¥å¤´æ¡éƒ¨åˆ†æå–
    if not keywords:
        headline_match = re.search(r'ä»Šæ—¥å¤´æ¡[ï¼š:]\s*(.+)', markdown)
        if headline_match:
            keywords.append(headline_match.group(1).strip()[:15])

    # 3. è¡¥å……å¸¸è§ AI å…³é”®è¯
    ai_keywords = ["Claude", "GPT", "OpenAI", "Anthropic", "AI Agent", "LLM"]
    for kw in ai_keywords:
        if kw.lower() in markdown.lower() and kw not in keywords:
            keywords.append(kw)
            if len(keywords) >= 4:
                break

    return keywords[:4]


def daily_task():
    """æ¯æ—¥å®šæ—¶ä»»åŠ¡"""
    logger.info("=" * 50)
    logger.info("å®šæ—¶ä»»åŠ¡è§¦å‘")
    logger.info("=" * 50)
    
    try:
        # 1. ç”Ÿæˆæ—¥æŠ¥
        agent = TechDigestAgent()
        digest = agent.run()
        
        # 2. å‘å¸ƒåˆ°å¾®ä¿¡ï¼ˆåªä½¿ç”¨ HTML æ ¼å¼ï¼‰
        if os.getenv("WECHAT_APP_ID") and os.getenv("WECHAT_APP_SECRET"):
            html_content = digest.get("html", "")

            if not html_content:
                logger.error("HTML å†…å®¹ä¸ºç©ºï¼Œæ‹’ç»å‘å¸ƒåˆ°å…¬ä¼—å·")
                logger.error("è¯·æ£€æŸ¥ Claude API è¿”å›çš„å†…å®¹æ˜¯å¦åŒ…å« [WECHAT_HTML] æ ‡ç­¾")
            else:
                publisher = WeChatPublisher()
                logger.info("ä½¿ç”¨ HTML æ ¼å¼å‘å¸ƒåˆ°å…¬ä¼—å·")
                today_short = datetime.now().strftime("%m.%d")
                # ä» markdown æå–å…³é”®è¯ç”¨äºå°é¢å›¾
                keywords = extract_keywords_from_markdown(digest.get("markdown", ""))
                logger.info(f"æå–åˆ°å…³é”®è¯: {keywords}")
                result = publisher.run(html_content, f"Tech Digest {today_short}", keywords)
                logger.info(f"å‘å¸ƒç»“æœ: {result}")
        else:
            logger.warning("æœªé…ç½®å¾®ä¿¡å…¬ä¼—å·å‡­è¯ï¼Œè·³è¿‡å‘å¸ƒ")
        
        logger.info("æ¯æ—¥ä»»åŠ¡å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)


def main():
    parser = argparse.ArgumentParser(description="Daily Tech Digest Agent")
    parser.add_argument("--schedule", action="store_true", help="å¯åŠ¨å®šæ—¶ä»»åŠ¡æ¨¡å¼")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼ˆåªç”Ÿæˆä¸å‘å¸ƒï¼‰")
    parser.add_argument("--time", default="08:00", help="å®šæ—¶æ‰§è¡Œæ—¶é—´ï¼ˆé»˜è®¤ 08:00ï¼‰")
    args = parser.parse_args()
    
    if args.schedule:
        # å®šæ—¶ä»»åŠ¡æ¨¡å¼
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯å¤© {args.time} æ‰§è¡Œ")
        schedule.every().day.at(args.time).do(daily_task)
        
        while True:
            schedule.run_pending()
            time.sleep(60)
    
    elif args.test:
        # æµ‹è¯•æ¨¡å¼
        logger.info("æµ‹è¯•æ¨¡å¼ï¼šåªç”Ÿæˆæ—¥æŠ¥ï¼Œä¸å‘å¸ƒ")
        agent = TechDigestAgent()
        digest = agent.run()
        print("\n=== ç”Ÿæˆç»“æœ ===")
        print(f"Markdown: {digest.get('md_path')}")
        print(f"HTML: {digest.get('html_path')}")
    
    else:
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        daily_task()


if __name__ == "__main__":
    main()
